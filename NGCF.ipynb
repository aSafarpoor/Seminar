{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aSafarpoor/Seminar/blob/main/NGCF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxkbi-1dPnKG",
        "outputId": "ca9dc6bb-07bb-4d82-ba93-471abd014cfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq434_X99S_b"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "phzv8AA5PwgL",
        "outputId": "8047a1fd-bfd8-4bb4-f403-70c2bef700bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/MSc/codes/NGCF\n",
            "dataPreprosessing.py  GCFmodel.py  __init__.py\trun.py\n",
            "DGLtest.py\t      GraphNCF\t   __pycache__\ttoyDataset\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/MSc/codes/NGCF\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDrivZKL6TeA"
      },
      "source": [
        "#DataPreprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tr1lsT15P_T4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "# movielens 1k\n",
        "\n",
        "class ML1K(Dataset):\n",
        "\n",
        "    def __init__(self,rt):\n",
        "        super(Dataset,self).__init__()\n",
        "        self.uId = list(rt['userId'])\n",
        "        self.iId = list(rt['itemId'])\n",
        "        self.rt = list(rt['rating'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.uId)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return (self.uId[item],self.iId[item],self.rt[item])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuADHdL66hRA"
      },
      "source": [
        "#load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC1Pb_Re8i6N"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi4nRNrVQIQ3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from os import path\n",
        "\n",
        "# load 100k data\n",
        "\n",
        "# path100k = path.dirname(__file__) + r'\\1K'\n",
        "path100k = 'toyDataset/1K/ml-100k'\n",
        "def load100KRatings():\n",
        "    df = pd.read_table(path100k+'/u.data',sep='\\t',names=['userId','itemId','rating','timestamp'])\n",
        "    return df\n",
        "\n",
        "def load100KItemSide():\n",
        "    import codecs\n",
        "    with codecs.open(path100k+'/u.item', 'r', 'utf-8', errors='ignore') as f:\n",
        "        movies = pd.read_table(f, delimiter='|', header=None,names=\"itemId| movie title | release date | video release date | IMDb URL | unknown | Action | Adventure | Animation | Children's | Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western \".split('|'))\n",
        "    return movies\n",
        "\n",
        "def load100kUserSide():\n",
        "    import codecs\n",
        "    with codecs.open(path100k + '/u.user', 'r', 'utf-8', errors='ignore') as f:\n",
        "        users = pd.read_table(f, delimiter='|', header=None,names=\"userId| age | gender | occupation | zip code\".split('|'))\n",
        "    return users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbdpRdhE6m0P"
      },
      "source": [
        "GCFmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ6KEdJE6bOn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module\n",
        "from scipy.sparse import coo_matrix\n",
        "from scipy.sparse import vstack\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# several models for recommendations\n",
        "\n",
        "# RMSE\n",
        "# SVD dim = 50 50 epoch RMSE = 0.931\n",
        "# GNCF dim = 64 layer = [64,64,64] nn = [128,64,32,] 50 epoch RMSE = 0.916/RMSE =0.914\n",
        "# NCF dim = 64 50 nn = [128,54,32] epoch 50 RMSE = 0.928\n",
        "\n",
        "class SVD(Module):\n",
        "\n",
        "    def __init__(self,userNum,itemNum,dim):\n",
        "        super(SVD, self).__init__()\n",
        "        self.uEmbd = nn.Embedding(userNum,dim)\n",
        "        self.iEmbd = nn.Embedding(itemNum,dim)\n",
        "        self.uBias = nn.Embedding(userNum,1)\n",
        "        self.iBias = nn.Embedding(itemNum,1)\n",
        "        self.overAllBias = nn.Parameter(torch.Tensor([0]))\n",
        "\n",
        "    def forward(self, userIdx,itemIdx):\n",
        "        uembd = self.uEmbd(userIdx)\n",
        "        iembd = self.iEmbd(itemIdx)\n",
        "        ubias = self.uBias(userIdx)\n",
        "        ibias = self.iBias(itemIdx)\n",
        "\n",
        "        biases = ubias + ibias + self.overAllBias\n",
        "        prediction = torch.sum(torch.mul(uembd,iembd),dim=1) + biases.flatten()\n",
        "\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXlODC4J6tmo"
      },
      "outputs": [],
      "source": [
        "class NCF(Module):\n",
        "\n",
        "    def __init__(self,userNum,itemNum,dim,layers=[128,64,32,8]):\n",
        "        super(NCF, self).__init__()\n",
        "        self.uEmbd = nn.Embedding(userNum,dim)\n",
        "        self.iEmbd = nn.Embedding(itemNum,dim)\n",
        "        self.fc_layers = torch.nn.ModuleList()\n",
        "        self.finalLayer = torch.nn.Linear(layers[-1],1)\n",
        "\n",
        "        for From,To in zip(layers[:-1],layers[1:]):\n",
        "            self.fc_layers.append(nn.Linear(From,To))\n",
        "\n",
        "    def forward(self, userIdx,itemIdx):\n",
        "        uembd = self.uEmbd(userIdx)\n",
        "        iembd = self.iEmbd(itemIdx)\n",
        "        embd = torch.cat([uembd, iembd], dim=1)\n",
        "        x = embd\n",
        "        for l in self.fc_layers:\n",
        "            x = l(x)\n",
        "            x = nn.ReLU()(x)\n",
        "\n",
        "        prediction = self.finalLayer(x)\n",
        "        return prediction.flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba5A5_j76xkD"
      },
      "outputs": [],
      "source": [
        "class GNNLayer(Module):\n",
        "\n",
        "    def __init__(self,inF,outF):\n",
        "\n",
        "        super(GNNLayer,self).__init__()\n",
        "        self.inF = inF\n",
        "        self.outF = outF\n",
        "        self.linear = torch.nn.Linear(in_features=inF,out_features=outF)\n",
        "        self.interActTransform = torch.nn.Linear(in_features=inF,out_features=outF)\n",
        "\n",
        "    def forward(self, laplacianMat,selfLoop,features):\n",
        "        # for GCF ajdMat is a (N+M) by (N+M) mat\n",
        "        # laplacianMat L = D^-1(A)D^-1 # 拉普拉斯矩阵\n",
        "        L1 = laplacianMat + selfLoop\n",
        "        L2 = laplacianMat.cuda()\n",
        "        L1 = L1.cuda()\n",
        "        inter_feature = torch.sparse.mm(L2,features)\n",
        "        inter_feature = torch.mul(inter_feature,features)\n",
        "\n",
        "        inter_part1 = self.linear(torch.sparse.mm(L1,features))\n",
        "        inter_part2 = self.interActTransform(torch.sparse.mm(L2,inter_feature))\n",
        "\n",
        "        return inter_part1+inter_part2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyMI3i8U61to"
      },
      "outputs": [],
      "source": [
        "class GCF(Module):\n",
        "\n",
        "    def __init__(self,userNum,itemNum,rt,embedSize=100,layers=[100,80,50],useCuda=True):\n",
        "\n",
        "        super(GCF,self).__init__()\n",
        "        self.useCuda = useCuda\n",
        "        self.userNum = userNum\n",
        "        self.itemNum = itemNum\n",
        "        self.uEmbd = nn.Embedding(userNum,embedSize)\n",
        "        self.iEmbd = nn.Embedding(itemNum,embedSize)\n",
        "        self.GNNlayers = torch.nn.ModuleList()\n",
        "        self.LaplacianMat = self.buildLaplacianMat(rt) # sparse format\n",
        "        self.leakyRelu = nn.LeakyReLU()\n",
        "        self.selfLoop = self.getSparseEye(self.userNum+self.itemNum)\n",
        "\n",
        "        self.transForm1 = nn.Linear(in_features=layers[-1]*(len(layers))*2,out_features=64)\n",
        "        self.transForm2 = nn.Linear(in_features=64,out_features=32)\n",
        "        self.transForm3 = nn.Linear(in_features=32,out_features=1)\n",
        "\n",
        "        for From,To in zip(layers[:-1],layers[1:]):\n",
        "            self.GNNlayers.append(GNNLayer(From,To))\n",
        "\n",
        "    def getSparseEye(self,num):\n",
        "        i = torch.LongTensor([[k for k in range(0,num)],[j for j in range(0,num)]])\n",
        "        val = torch.FloatTensor([1]*num)\n",
        "        return torch.sparse.FloatTensor(i,val)\n",
        "\n",
        "    def buildLaplacianMat(self,rt):\n",
        "\n",
        "        rt_item = rt['itemId'] + self.userNum\n",
        "        uiMat = coo_matrix((rt['rating'], (rt['userId'], rt['itemId'])))\n",
        "\n",
        "        uiMat_upperPart = coo_matrix((rt['rating'], (rt['userId'], rt_item)))\n",
        "        uiMat = uiMat.transpose()\n",
        "        uiMat.resize((self.itemNum, self.userNum + self.itemNum))\n",
        "\n",
        "        A = sparse.vstack([uiMat_upperPart,uiMat])\n",
        "        selfLoop = sparse.eye(self.userNum+self.itemNum)\n",
        "        sumArr = (A>0).sum(axis=1)\n",
        "        diag = list(np.array(sumArr.flatten())[0])\n",
        "        diag = np.power(diag,-0.5)\n",
        "        D = sparse.diags(diag)\n",
        "        L = D * A * D\n",
        "        L = sparse.coo_matrix(L)\n",
        "        row = L.row\n",
        "        col = L.col\n",
        "        i = torch.LongTensor([row,col])\n",
        "        data = torch.FloatTensor(L.data)\n",
        "        SparseL = torch.sparse.FloatTensor(i,data)\n",
        "        return SparseL\n",
        "\n",
        "    def getFeatureMat(self):\n",
        "        uidx = torch.LongTensor([i for i in range(self.userNum)])\n",
        "        iidx = torch.LongTensor([i for i in range(self.itemNum)])\n",
        "        if self.useCuda == True:\n",
        "            uidx = uidx.cuda()\n",
        "            iidx = iidx.cuda()\n",
        "\n",
        "        userEmbd = self.uEmbd(uidx)\n",
        "        itemEmbd = self.iEmbd(iidx)\n",
        "        features = torch.cat([userEmbd,itemEmbd],dim=0)\n",
        "        return features\n",
        "\n",
        "    def forward(self,userIdx,itemIdx):\n",
        "\n",
        "        itemIdx = itemIdx + self.userNum\n",
        "        userIdx = list(userIdx.cpu().data)\n",
        "        itemIdx = list(itemIdx.cpu().data)\n",
        "        # gcf data propagation\n",
        "        features = self.getFeatureMat()\n",
        "        finalEmbd = features.clone()\n",
        "        for gnn in self.GNNlayers:\n",
        "            features = gnn(self.LaplacianMat,self.selfLoop,features)\n",
        "            features = nn.ReLU()(features)\n",
        "            finalEmbd = torch.cat([finalEmbd,features.clone()],dim=1)\n",
        "\n",
        "        userEmbd = finalEmbd[userIdx]\n",
        "        itemEmbd = finalEmbd[itemIdx]\n",
        "        embd = torch.cat([userEmbd,itemEmbd],dim=1)\n",
        "\n",
        "        embd = nn.ReLU()(self.transForm1(embd))\n",
        "        embd = self.transForm2(embd)\n",
        "        embd = self.transForm3(embd)\n",
        "        prediction = embd.flatten()\n",
        "\n",
        "        return prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVNe_gRV65Vg"
      },
      "outputs": [],
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    # from toyDataset.loaddata import load100KRatings\n",
        "\n",
        "    rt = load100KRatings()\n",
        "    userNum = rt['userId'].max()\n",
        "    itemNum = rt['itemId'].max()\n",
        "\n",
        "    rt['userId'] = rt['userId'] - 1\n",
        "    rt['itemId'] = rt['itemId'] - 1\n",
        "    gcf = GCF(userNum,itemNum,rt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVzk76Wg67pY"
      },
      "source": [
        "# run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2JX8m1i67yQ",
        "outputId": "b414fcd2-4ec1-4547-c2cf-3ee14e2ee39d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [02:11<00:00,  1.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9098504781723022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "from scipy.sparse import coo_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import diag\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch.optim import Adam\n",
        "from torch.nn import MSELoss\n",
        "from tqdm import tqdm\n",
        "\n",
        "rt = load100KRatings()\n",
        "userNum = rt['userId'].max()\n",
        "itemNum = rt['itemId'].max()\n",
        "\n",
        "rt['userId'] = rt['userId'] - 1\n",
        "rt['itemId'] = rt['itemId'] - 1\n",
        "#\n",
        "# rtIt = rt['itemId'] + userNum\n",
        "# uiMat = coo_matrix((rt['rating'],(rt['userId'],rt['itemId'])))\n",
        "# uiMat_upperPart = coo_matrix((rt['rating'],(rt['userId'],rtIt)))\n",
        "# uiMat = uiMat.transpose()\n",
        "# uiMat.resize((itemNum,userNum+itemNum))\n",
        "# uiMat = uiMat.todense()\n",
        "# uiMat_t = uiMat.transpose()\n",
        "# zeros1 = np.zeros((userNum,userNum))\n",
        "# zeros2 = np.zeros((itemNum,itemNum))\n",
        "#\n",
        "# p1 = np.concatenate([zeros1,uiMat],axis=1)\n",
        "# p2 = np.concatenate([uiMat_t,zeros2],axis=1)\n",
        "# mat = np.concatenate([p1,p2])\n",
        "#\n",
        "# count = (mat > 0)+0\n",
        "# diagval = np.array(count.sum(axis=0))[0]\n",
        "# diagval = np.power(diagval,(-1/2))\n",
        "# D_ = diag(diagval)\n",
        "#\n",
        "# L = np.dot(np.dot(D_,mat),D_)\n",
        "#\n",
        "para = {\n",
        "    'epoch':100,\n",
        "    'lr':0.01,\n",
        "    'batch_size':2048,\n",
        "    'train':0.8\n",
        "}\n",
        "\n",
        "ds = ML1K(rt)\n",
        "trainLen = int(para['train']*len(ds))\n",
        "train,test = random_split(ds,[trainLen,len(ds)-trainLen])\n",
        "dl = DataLoader(train,batch_size=para['batch_size'],shuffle=True,pin_memory=True)\n",
        "\n",
        "model = GCF(userNum, itemNum, rt, 80, layers=[80,80,]).cuda()\n",
        "# model = SVD(userNum,itemNum,50).cuda()\n",
        "# model = NCF(userNum,itemNum,64,layers=[128,64,32,16,8]).cuda()\n",
        "optim = Adam(model.parameters(), lr=para['lr'])\n",
        "lossfn = MSELoss()\n",
        "\n",
        "loss_list_reg = []\n",
        "for i in tqdm(range(para['epoch'])):\n",
        "    # print('epoch num: ',i+1)\n",
        "    for id,batch in enumerate(dl):\n",
        "        # print('epoch:',i,' batch:',id)\n",
        "        optim.zero_grad()\n",
        "        prediction = model(batch[0].cuda(), batch[1].cuda())\n",
        "        loss = lossfn(batch[2].float().cuda(),prediction)\n",
        "\n",
        "        l2_lambda = 0.01\n",
        "        l2_reg = torch.tensor(0.).cuda()\n",
        "        for param in model.parameters():\n",
        "            l2_reg += torch.norm(param)\n",
        "        loss += l2_lambda * l2_reg\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "    try:\n",
        "        for data in testdl:\n",
        "            testdl = DataLoader(test,batch_size=len(test),)\n",
        "            prediction = model(data[0].cuda(),data[1].cuda())\n",
        "            loss = lossfn(data[2].float().cuda(),prediction)\n",
        "            loss_list_reg.append(round(loss.item(),3))\n",
        "    except:\n",
        "        print('some little error')\n",
        "\n",
        "\n",
        "testdl = DataLoader(test,batch_size=len(test),)\n",
        "for data in testdl:\n",
        "    prediction = model(data[0].cuda(),data[1].cuda())\n",
        "    loss = lossfn(data[2].float().cuda(),prediction)\n",
        "\n",
        "\n",
        "print(loss.item()) # MSEloss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR6ltyBOGFtW"
      },
      "outputs": [],
      "source": [
        "# plt.plot(range(len(loss_list_wd)),loss_list_wd,label='weight decay')\n",
        "# plt.plot(range(len(loss_list_no_reg)),loss_list_no_reg,label='without reguliozer')\n",
        "# plt.plot(range(len(loss_list_reg)),loss_list_reg,label = 'with regulizer')\n",
        "# plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH_SO3KwGMTe"
      },
      "outputs": [],
      "source": [
        "testdl = DataLoader(test,batch_size=len(test),)\n",
        "for data in testdl:\n",
        "    d0,d1= data[0],data[1]\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L48CTKWyb3Pg"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NGCF.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMC95nSfRWbkQ2+b0A7IlfH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}